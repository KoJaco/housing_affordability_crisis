{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a060dc",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Where I found the data and instructions on data elements (see documentation)\n",
    "- https://valuation.property.nsw.gov.au/embed/propertySalesInformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbdd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b38ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: ../data\n",
      "Output directory: ../data/parquet\n",
      "Years to process: [2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"../data/parquet\"\n",
    "YEARS = range(2005, 2026)  # covering 2005-2025\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Years to process: {list(YEARS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2702fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file: ../data/2024/081_SALES_DATA_NNME_30122024.DAT\n",
      "\n",
      "1: A;RTSALEDATA;081;20241230 01:07;VALNET;\n",
      "2: B;081;2940952;1;20241230 01:07;;;48;VALENTI CRES;KELLYVILLE;2155;679;M;20241031;20241219;2202000;R2;\n",
      "3: C;081;2940952;1;20241230 01:07;106/1041473;\n",
      "4: D;081;2940952;1;20241230 01:07;P;;;;;;\n",
      "5: D;081;2940952;1;20241230 01:07;P;;;;;;\n",
      "6: D;081;2940952;1;20241230 01:07;V;;;;;;\n",
      "7: D;081;2940952;1;20241230 01:07;V;;;;;;\n",
      "8: B;081;2987409;2;20241230 01:07;;;82;EDGEWATER DR;BELLA VISTA;2153;640;M;20240627;20241220;2805000;R2\n",
      "9: C;081;2987409;2;20241230 01:07;1133/1046919;\n",
      "10: D;081;2987409;2;20241230 01:07;P;;;;;;\n",
      "11: D;081;2987409;2;20241230 01:07;P;;;;;;\n",
      "12: D;081;2987409;2;20241230 01:07;V;;;;;;\n",
      "13: B;081;3014211;3;20241230 01:07;VICTORIA GARDENS;15;11;HARRINGTON AVE;CASTLE HILL;2154;;;20241012;202\n",
      "14: C;081;3014211;3;20241230 01:07;15/SP70002;\n",
      "15: D;081;3014211;3;20241230 01:07;P;;;;;;\n",
      "16: D;081;3014211;3;20241230 01:07;P;;;;;;\n",
      "17: D;081;3014211;3;20241230 01:07;V;;;;;;\n",
      "18: D;081;3014211;3;20241230 01:07;V;;;;;;\n",
      "19: B;081;4153379;4;20241230 01:07;;;50;SOLSTICE ST;BOX HILL;2765;517;M;20241115;20241220;1475000;R2;R;R\n",
      "20: C;081;4153379;4;20241230 01:07;306/1231910;\n"
     ]
    }
   ],
   "source": [
    "# checking out the file format - semicolon separated\n",
    "sample_file = glob.glob(f\"{DATA_DIR}/2024/*.DAT\")[0]\n",
    "print(f\"Sample file: {sample_file}\\n\")\n",
    "\n",
    "# peek at first 20 lines to see structure\n",
    "with open(sample_file, 'r', encoding='latin-1') as f:\n",
    "    lines = f.readlines()[:20]\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        print(f\"{i}: {line.strip()[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11818fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B Record Structure:\n",
      "Total columns: 25\n",
      "  Column 0: B\n",
      "  Column 1: 001\n",
      "  Column 2: 2857799\n",
      "  Column 3: 1\n",
      "  Column 4: 20240101 01:07\n",
      "  Column 7: 176\n",
      "  Column 8: LAKE RD\n",
      "  Column 9: ELRINGTON\n",
      "  Column 10: 2325\n",
      "  Column 11: 25.15\n",
      "  Column 12: H\n",
      "  Column 13: 20231219\n",
      "  Column 14: 20231222\n",
      "  Column 15: 1330000\n",
      "  Column 16: RU2\n",
      "  Column 17: R\n",
      "  Column 18: RESIDENCE\n",
      "  Column 20: AAI\n",
      "  Column 22: 0\n",
      "  Column 23: AS334275\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# B records contain the main sales data - let's see what fields we have\n",
    "sample_b_record = \"B;001;2857799;1;20240101 01:07;;;176;LAKE RD;ELRINGTON;2325;25.15;H;20231219;20231222;1330000;RU2;R;RESIDENCE;;AAI;;0;AS334275;\"\n",
    "parts = sample_b_record.strip().split(';')\n",
    "print(\"B Record Structure:\")\n",
    "print(f\"Total columns: {len(parts)}\")\n",
    "for i, part in enumerate(parts):\n",
    "    if part:  # skip empty fields\n",
    "        print(f\"  Column {i}: {part}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1534ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dat_file(filepath):\n",
    "    \"\"\"\n",
    "    Parses .DAT files and extracts B records (the actual sales data).\n",
    "    Returns a list of dicts with parsed fields.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('A') or line.startswith('C') or line.startswith('D'):\n",
    "                    continue\n",
    "                \n",
    "                if line.startswith('B'):\n",
    "                    parts = line.split(';')\n",
    "                    if len(parts) < 15:\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # extracting fields - keeping raw record for debugging if needed\n",
    "                        record = {\n",
    "                            'record_type': parts[0] if len(parts) > 0 else None,\n",
    "                            'district_code': parts[1] if len(parts) > 1 else None,\n",
    "                            'property_id': parts[2] if len(parts) > 2 else None,\n",
    "                            'sale_counter': parts[3] if len(parts) > 3 else None,\n",
    "                            'download_timestamp': parts[4] if len(parts) > 4 else None,\n",
    "                            'property_name': parts[5] if len(parts) > 5 else None,\n",
    "                            'property_unit_number': parts[6] if len(parts) > 6 else None,\n",
    "                            'property_house_number': parts[7] if len(parts) > 7 else None,\n",
    "                            'property_street_name': parts[8] if len(parts) > 8 else None,\n",
    "                            'property_locality': parts[9] if len(parts) > 9 else None,\n",
    "                            'property_post_code': parts[10] if len(parts) > 10 else None,\n",
    "                            'area': parts[11] if len(parts) > 11 else None,\n",
    "                            'area_type': parts[12] if len(parts) > 12 else None,\n",
    "                            'contract_date': parts[13] if len(parts) > 13 else None,\n",
    "                            'settlement_date': parts[14] if len(parts) > 14 else None,\n",
    "                            'purchase_price': parts[15] if len(parts) > 15 else None,\n",
    "                            'zoning': parts[16] if len(parts) > 16 else None,\n",
    "                            'nature_of_property': parts[17] if len(parts) > 17 else None,\n",
    "                            'primary_purpose': parts[18] if len(parts) > 18 else None,\n",
    "                            'strata_lot_number': parts[19] if len(parts) > 19 else None,\n",
    "                            'component_code': parts[20] if len(parts) > 20 else None,\n",
    "                            'sale_code': parts[21] if len(parts) > 21 else None,\n",
    "                            'percent_interest_of_sale': parts[22] if len(parts) > 22 else None,\n",
    "                            'dealing_number': parts[23] if len(parts) > 23 else None,\n",
    "                            'source_file': os.path.basename(filepath),\n",
    "                            'raw_record': line\n",
    "                        }\n",
    "                        records.append(record)\n",
    "                    except Exception as e:\n",
    "                        # Skip malformed records\n",
    "                        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "    \n",
    "    return records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b64a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 201 records from test file\n",
      "\n",
      "First record:\n",
      "  record_type: B\n",
      "  district_code: 081\n",
      "  property_id: 2940952\n",
      "  sale_counter: 1\n",
      "  download_timestamp: 20241230 01:07\n",
      "  property_name: \n",
      "  property_unit_number: \n",
      "  property_house_number: 48\n",
      "  property_street_name: VALENTI CRES\n",
      "  property_locality: KELLYVILLE\n",
      "  property_post_code: 2155\n",
      "  area: 679\n",
      "  area_type: M\n",
      "  contract_date: 20241031\n",
      "  settlement_date: 20241219\n",
      "  purchase_price: 2202000\n",
      "  zoning: R2\n",
      "  nature_of_property: R\n",
      "  primary_purpose: RESIDENCE\n",
      "  strata_lot_number: \n",
      "  component_code: AYY\n",
      "  sale_code: \n",
      "  percent_interest_of_sale: 0\n",
      "  dealing_number: AU705676\n",
      "  source_file: 081_SALES_DATA_NNME_30122024.DAT\n"
     ]
    }
   ],
   "source": [
    "# quick test on one file\n",
    "test_file = glob.glob(f\"{DATA_DIR}/2024/*.DAT\")[0]\n",
    "test_records = parse_dat_file(test_file)\n",
    "print(f\"Parsed {len(test_records)} records from test file\")\n",
    "if test_records:\n",
    "    print(\"\\nFirst record:\")\n",
    "    for key, value in test_records[0].items():\n",
    "        if key != 'raw_record':  # uncomment to see raw record if debugging\n",
    "            print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275a2507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(year, cache_mismatched=True):\n",
    "    \"\"\"\n",
    "    Processes all .DAT files for a year, returns DataFrame and cached records.\n",
    "    cache_df contains records with settlement dates from other years.\n",
    "    \"\"\"\n",
    "    year_dir = f\"{DATA_DIR}/{year}\"\n",
    "    \n",
    "    if not os.path.exists(year_dir):\n",
    "        print(f\"  Year {year}: Directory not found\")\n",
    "        return None, None\n",
    "    \n",
    "    # grab all .DAT files (case insensitive)\n",
    "    dat_files = glob.glob(f\"{year_dir}/*.DAT\") + glob.glob(f\"{year_dir}/*.dat\")\n",
    "    \n",
    "    if not dat_files:\n",
    "        print(f\"  Year {year}: No .DAT files found\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"  Year {year}: Found {len(dat_files)} .DAT file(s)\")\n",
    "    \n",
    "    all_records = []\n",
    "    for dat_file in dat_files:\n",
    "        records = parse_dat_file(dat_file)\n",
    "        all_records.extend(records)\n",
    "    \n",
    "    if not all_records:\n",
    "        print(f\"  Year {year}: No records extracted\")\n",
    "        return None, None\n",
    "    \n",
    "    df = pd.DataFrame(all_records)\n",
    "    \n",
    "    # date conversions\n",
    "    if 'settlement_date' in df.columns:\n",
    "        df['settlement_date'] = pd.to_datetime(\n",
    "            df['settlement_date'], \n",
    "            format='%Y%m%d', \n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    if 'contract_date' in df.columns:\n",
    "        df['contract_date'] = pd.to_datetime(\n",
    "            df['contract_date'], \n",
    "            format='%Y%m%d',  # CCYYMMDD format\n",
    "            errors='coerce'\n",
    "        )\n",
    "    \n",
    "    if 'purchase_price' in df.columns:\n",
    "        df['purchase_price'] = pd.to_numeric(df['purchase_price'], errors='coerce')\n",
    "    \n",
    "    # split by settlement date year - some files have records from other years\n",
    "    cache_df = None\n",
    "    if 'settlement_date' in df.columns and cache_mismatched:\n",
    "        initial_count = len(df)\n",
    "        df_matched = df[df['settlement_date'].dt.year == year].copy()\n",
    "        df_cache = df[df['settlement_date'].dt.year != year].copy()\n",
    "        \n",
    "        if len(df_cache) > 0:\n",
    "            cache_df = df_cache\n",
    "            print(f\"  Year {year}: Cached {len(df_cache)} records with settlement dates from other years\")\n",
    "        \n",
    "        df = df_matched\n",
    "        matched_count = len(df)\n",
    "        if initial_count != matched_count:\n",
    "            print(f\"  Year {year}: Matched {matched_count} records for {year}, cached {len(df_cache) if cache_df is not None else 0} for other years\")\n",
    "    \n",
    "    if 'settlement_date' in df.columns and len(df) > 0:\n",
    "        df = df.sort_values('settlement_date', na_position='last')\n",
    "    \n",
    "    print(f\"  Year {year}: Extracted {len(df)} records\")\n",
    "    if 'settlement_date' in df.columns and len(df) > 0:\n",
    "        print(f\"  Year {year}: Date range: {df['settlement_date'].min()} to {df['settlement_date'].max()}\")\n",
    "    \n",
    "    return df, cache_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3775613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settlement date year distribution (before filtering):\n",
      "settlement_date\n",
      "2002      1\n",
      "2004      1\n",
      "2005    121\n",
      "2006    332\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 455\n",
      "Records with valid settlement_date: 455\n",
      "Records with invalid settlement_date: 0\n",
      "\n",
      "Sample records that would be filtered out:\n",
      "   settlement_date property_locality purchase_price  \\\n",
      "2       2005-10-28             MANLY        1380000   \n",
      "10      2005-09-01         FAIRLIGHT         385000   \n",
      "27      2005-12-20             MANLY        1202000   \n",
      "36      2005-12-23             MANLY         580546   \n",
      "38      2005-09-23             MANLY          13000   \n",
      "\n",
      "                         source_file  \n",
      "2   086_SALES_DATA_NNME_01042006.DAT  \n",
      "10  086_SALES_DATA_NNME_01042006.DAT  \n",
      "27  086_SALES_DATA_NNME_01042006.DAT  \n",
      "36  086_SALES_DATA_NNME_01042006.DAT  \n",
      "38  086_SALES_DATA_NNME_01042006.DAT  \n"
     ]
    }
   ],
   "source": [
    "# noticed lots of records getting filtered out - checking what's happening\n",
    "test_year = 2006\n",
    "year_dir = f\"{DATA_DIR}/{test_year}\"\n",
    "dat_files = glob.glob(f\"{year_dir}/*.DAT\") + glob.glob(f\"{year_dir}/*.dat\")\n",
    "\n",
    "all_records = []\n",
    "for dat_file in dat_files[:10]:  # sample first 10 files\n",
    "    records = parse_dat_file(dat_file)\n",
    "    all_records.extend(records)\n",
    "\n",
    "df_diag = pd.DataFrame(all_records)\n",
    "df_diag['settlement_date'] = pd.to_datetime(df_diag['settlement_date'], format='%Y%m%d', errors='coerce')\n",
    "\n",
    "print(\"Settlement date year distribution (before filtering):\")\n",
    "print(df_diag['settlement_date'].dt.year.value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nTotal records: {len(df_diag)}\")\n",
    "print(f\"Records with valid settlement_date: {df_diag['settlement_date'].notna().sum()}\")\n",
    "print(f\"Records with invalid settlement_date: {df_diag['settlement_date'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nSample records that would be filtered out:\")\n",
    "filtered_out = df_diag[df_diag['settlement_date'].dt.year != test_year].head(5)\n",
    "print(filtered_out[['settlement_date', 'property_locality', 'purchase_price', 'source_file']])\n",
    "\n",
    "#! The result? We have many records added in the processed year from previous years. We must include these still in our dataset. My method is to cache filtered out records according to settlement date. After processing all our years with our filter in place, we check through our cache and merge these back into the year .parquet file they belong to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b156267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with year 2024...\n",
      "  Year 2024: Found 6461 .DAT file(s)\n",
      "  Year 2024: Cached 3743 records with settlement dates from other years\n",
      "  Year 2024: Matched 213114 records for 2024, cached 3743 for other years\n",
      "  Year 2024: Extracted 213114 records\n",
      "  Year 2024: Date range: 2024-01-02 00:00:00 to 2024-12-24 00:00:00\n",
      "\n",
      "DataFrame shape: (213114, 26)\n",
      "\n",
      "Columns: ['record_type', 'district_code', 'property_id', 'sale_counter', 'download_timestamp', 'property_name', 'property_unit_number', 'property_house_number', 'property_street_name', 'property_locality', 'property_post_code', 'area', 'area_type', 'contract_date', 'settlement_date', 'purchase_price', 'zoning', 'nature_of_property', 'primary_purpose', 'strata_lot_number', 'component_code', 'sale_code', 'percent_interest_of_sale', 'dealing_number', 'source_file', 'raw_record']\n",
      "\n",
      "First few rows:\n",
      "      record_type district_code property_id sale_counter download_timestamp  \\\n",
      "46066           B           224     4523223           16     20240108 01:07   \n",
      "46064           B           224     4523223           14     20240108 01:07   \n",
      "46063           B           224     4523223           13     20240108 01:07   \n",
      "46062           B           224     4523223           12     20240108 01:07   \n",
      "58164           B           082      635043            2     20240108 01:05   \n",
      "\n",
      "      property_name property_unit_number property_house_number  \\\n",
      "46066                                205                    12   \n",
      "46064                                109                    12   \n",
      "46063                                  6                    12   \n",
      "46062                                  5                    12   \n",
      "58164                                                     64 A   \n",
      "\n",
      "      property_street_name property_locality  ... zoning nature_of_property  \\\n",
      "46066          CARSON LANE          ST MARYS  ...                         R   \n",
      "46064          CARSON LANE          ST MARYS  ...                         R   \n",
      "46063          CARSON LANE          ST MARYS  ...                         R   \n",
      "46062          CARSON LANE          ST MARYS  ...                         R   \n",
      "58164            CLARKE RD           HORNSBY  ...     R2                  R   \n",
      "\n",
      "      primary_purpose strata_lot_number component_code  sale_code  \\\n",
      "46066       RESIDENCE               175                             \n",
      "46064       RESIDENCE               168                             \n",
      "46063       RESIDENCE               154                             \n",
      "46062       RESIDENCE               153                             \n",
      "58164       RESIDENCE                              REC              \n",
      "\n",
      "      percent_interest_of_sale dealing_number  \\\n",
      "46066                        0       AT739398   \n",
      "46064                        0       AT739412   \n",
      "46063                        0       AT739370   \n",
      "46062                        0       AT739384   \n",
      "58164                        0       AT738866   \n",
      "\n",
      "                            source_file  \\\n",
      "46066  224_SALES_DATA_NNME_08012024.DAT   \n",
      "46064  224_SALES_DATA_NNME_08012024.DAT   \n",
      "46063  224_SALES_DATA_NNME_08012024.DAT   \n",
      "46062  224_SALES_DATA_NNME_08012024.DAT   \n",
      "58164  082_SALES_DATA_NNME_08012024.DAT   \n",
      "\n",
      "                                              raw_record  \n",
      "46066  B;224;4523223;16;20240108 01:07;;205;12;CARSON...  \n",
      "46064  B;224;4523223;14;20240108 01:07;;109;12;CARSON...  \n",
      "46063  B;224;4523223;13;20240108 01:07;;6;12;CARSON L...  \n",
      "46062  B;224;4523223;12;20240108 01:07;;5;12;CARSON L...  \n",
      "58164  B;082;635043;2;20240108 01:05;;;64 A;CLARKE RD...  \n",
      "\n",
      "[5 rows x 26 columns]\n",
      "\n",
      "Data types:\n",
      "record_type                         object\n",
      "district_code                       object\n",
      "property_id                         object\n",
      "sale_counter                        object\n",
      "download_timestamp                  object\n",
      "property_name                       object\n",
      "property_unit_number                object\n",
      "property_house_number               object\n",
      "property_street_name                object\n",
      "property_locality                   object\n",
      "property_post_code                  object\n",
      "area                                object\n",
      "area_type                           object\n",
      "contract_date               datetime64[ns]\n",
      "settlement_date             datetime64[ns]\n",
      "purchase_price                       int64\n",
      "zoning                              object\n",
      "nature_of_property                  object\n",
      "primary_purpose                     object\n",
      "strata_lot_number                   object\n",
      "component_code                      object\n",
      "sale_code                           object\n",
      "percent_interest_of_sale            object\n",
      "dealing_number                      object\n",
      "source_file                         object\n",
      "raw_record                          object\n",
      "dtype: object\n",
      "\n",
      "Missing values:\n",
      "record_type                  0\n",
      "district_code                0\n",
      "property_id                  0\n",
      "sale_counter                 0\n",
      "download_timestamp           0\n",
      "property_name                0\n",
      "property_unit_number         0\n",
      "property_house_number        0\n",
      "property_street_name         0\n",
      "property_locality            0\n",
      "property_post_code           0\n",
      "area                         0\n",
      "area_type                    0\n",
      "contract_date               46\n",
      "settlement_date              0\n",
      "purchase_price               0\n",
      "zoning                       0\n",
      "nature_of_property           0\n",
      "primary_purpose              0\n",
      "strata_lot_number            0\n",
      "component_code               0\n",
      "sale_code                    0\n",
      "percent_interest_of_sale     0\n",
      "dealing_number               0\n",
      "source_file                  0\n",
      "raw_record                   0\n",
      "dtype: int64\n",
      "\n",
      "Cached records shape: (3743, 26)\n",
      "Cached records settlement year distribution:\n",
      "settlement_date\n",
      "1992       1\n",
      "2001       2\n",
      "2002       4\n",
      "2003       2\n",
      "2004       5\n",
      "2005       7\n",
      "2006       6\n",
      "2007       7\n",
      "2008       6\n",
      "2009      11\n",
      "2010       8\n",
      "2011      12\n",
      "2012       5\n",
      "2013      14\n",
      "2014       9\n",
      "2015       9\n",
      "2016      11\n",
      "2017      20\n",
      "2018      21\n",
      "2019      12\n",
      "2020       9\n",
      "2021      35\n",
      "2022      36\n",
      "2023    3491\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# testing on one year first - backup your data before running!\n",
    "test_year = 2024\n",
    "print(f\"Testing with year {test_year}...\")\n",
    "test_df, test_cache = process_year(test_year, cache_mismatched=True)\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"\\nDataFrame shape: {test_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(test_df.head())\n",
    "    print(f\"\\nData types:\")\n",
    "    print(test_df.dtypes)\n",
    "    print(f\"\\nMissing values:\")\n",
    "    print(test_df.isnull().sum())\n",
    "    \n",
    "if test_cache is not None:\n",
    "    print(f\"\\nCached records shape: {test_cache.shape}\")\n",
    "    print(f\"Cached records settlement year distribution:\")\n",
    "    print(test_cache['settlement_date'].dt.year.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b932e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing all years...\n",
      "\n",
      "  Year 2005: Found 3671 .DAT file(s)\n",
      "  Year 2005: Cached 24713 records with settlement dates from other years\n",
      "  Year 2005: Matched 159407 records for 2005, cached 24713 for other years\n",
      "  Year 2005: Extracted 159407 records\n",
      "  Year 2005: Date range: 2005-01-01 00:00:00 to 2005-12-22 00:00:00\n",
      "  Year 2005: Saved using fastparquet engine\n",
      "  Year 2005: Saved to ../data/parquet/2005.parquet\n",
      "\n",
      "  Year 2006: Found 3648 .DAT file(s)\n",
      "  Year 2006: Cached 22446 records with settlement dates from other years\n",
      "  Year 2006: Matched 162764 records for 2006, cached 22446 for other years\n",
      "  Year 2006: Extracted 162764 records\n",
      "  Year 2006: Date range: 2006-01-01 00:00:00 to 2006-12-18 00:00:00\n",
      "  Year 2006: Saved using fastparquet engine\n",
      "  Year 2006: Saved to ../data/parquet/2006.parquet\n",
      "\n",
      "  Year 2007: Found 3633 .DAT file(s)\n",
      "  Year 2007: Cached 21054 records with settlement dates from other years\n",
      "  Year 2007: Matched 187111 records for 2007, cached 21054 for other years\n",
      "  Year 2007: Extracted 187111 records\n",
      "  Year 2007: Date range: 2007-01-01 00:00:00 to 2007-12-21 00:00:00\n",
      "  Year 2007: Saved using fastparquet engine\n",
      "  Year 2007: Saved to ../data/parquet/2007.parquet\n",
      "\n",
      "  Year 2008: Found 6616 .DAT file(s)\n",
      "  Year 2008: Cached 22354 records with settlement dates from other years\n",
      "  Year 2008: Matched 170222 records for 2008, cached 22354 for other years\n",
      "  Year 2008: Extracted 170222 records\n",
      "  Year 2008: Date range: 2008-01-01 00:00:00 to 2008-12-10 00:00:00\n",
      "  Year 2008: Saved using fastparquet engine\n",
      "  Year 2008: Saved to ../data/parquet/2008.parquet\n",
      "\n",
      "  Year 2009: Found 7710 .DAT file(s)\n",
      "  Year 2009: Cached 21609 records with settlement dates from other years\n",
      "  Year 2009: Matched 176370 records for 2009, cached 21609 for other years\n",
      "  Year 2009: Extracted 176370 records\n",
      "  Year 2009: Date range: 2009-01-01 00:00:00 to 2009-12-14 00:00:00\n",
      "  Year 2009: Saved using fastparquet engine\n",
      "  Year 2009: Saved to ../data/parquet/2009.parquet\n",
      "\n",
      "  Year 2010: Found 7515 .DAT file(s)\n",
      "  Year 2010: Cached 23764 records with settlement dates from other years\n",
      "  Year 2010: Matched 161520 records for 2010, cached 23764 for other years\n",
      "  Year 2010: Extracted 161520 records\n",
      "  Year 2010: Date range: 2010-01-01 00:00:00 to 2010-12-21 00:00:00\n",
      "  Year 2010: Saved using fastparquet engine\n",
      "  Year 2010: Saved to ../data/parquet/2010.parquet\n",
      "\n",
      "  Year 2011: Found 7514 .DAT file(s)\n",
      "  Year 2011: Cached 20024 records with settlement dates from other years\n",
      "  Year 2011: Matched 154945 records for 2011, cached 20024 for other years\n",
      "  Year 2011: Extracted 154945 records\n",
      "  Year 2011: Date range: 2011-01-01 00:00:00 to 2011-12-21 00:00:00\n",
      "  Year 2011: Saved using fastparquet engine\n",
      "  Year 2011: Saved to ../data/parquet/2011.parquet\n",
      "\n",
      "  Year 2012: Found 7624 .DAT file(s)\n",
      "  Year 2012: Cached 18157 records with settlement dates from other years\n",
      "  Year 2012: Matched 156499 records for 2012, cached 18157 for other years\n",
      "  Year 2012: Extracted 156499 records\n",
      "  Year 2012: Date range: 2012-01-01 00:00:00 to 2012-12-21 00:00:00\n",
      "  Year 2012: Saved using fastparquet engine\n",
      "  Year 2012: Saved to ../data/parquet/2012.parquet\n",
      "\n",
      "  Year 2013: Found 7516 .DAT file(s)\n",
      "  Year 2013: Cached 16211 records with settlement dates from other years\n",
      "  Year 2013: Matched 179097 records for 2013, cached 16211 for other years\n",
      "  Year 2013: Extracted 179097 records\n",
      "  Year 2013: Date range: 2013-01-01 00:00:00 to 2013-12-23 00:00:00\n",
      "  Year 2013: Saved using fastparquet engine\n",
      "  Year 2013: Saved to ../data/parquet/2013.parquet\n",
      "\n",
      "  Year 2014: Found 7546 .DAT file(s)\n",
      "  Year 2014: Cached 18325 records with settlement dates from other years\n",
      "  Year 2014: Matched 199967 records for 2014, cached 18325 for other years\n",
      "  Year 2014: Extracted 199967 records\n",
      "  Year 2014: Date range: 2014-01-01 00:00:00 to 2014-12-24 00:00:00\n",
      "  Year 2014: Saved using fastparquet engine\n",
      "  Year 2014: Saved to ../data/parquet/2014.parquet\n",
      "\n",
      "  Year 2015: Found 7615 .DAT file(s)\n",
      "  Year 2015: Cached 27162 records with settlement dates from other years\n",
      "  Year 2015: Matched 205173 records for 2015, cached 27162 for other years\n",
      "  Year 2015: Extracted 205173 records\n",
      "  Year 2015: Date range: 2015-01-01 00:00:00 to 2015-12-24 00:00:00\n",
      "  Year 2015: Saved using fastparquet engine\n",
      "  Year 2015: Saved to ../data/parquet/2015.parquet\n",
      "\n",
      "  Year 2016: Found 7554 .DAT file(s)\n",
      "  Year 2016: Cached 19158 records with settlement dates from other years\n",
      "  Year 2016: Matched 208214 records for 2016, cached 19158 for other years\n",
      "  Year 2016: Extracted 208214 records\n",
      "  Year 2016: Date range: 2016-01-01 00:00:00 to 2016-12-22 00:00:00\n",
      "  Year 2016: Saved using fastparquet engine\n",
      "  Year 2016: Saved to ../data/parquet/2016.parquet\n",
      "\n",
      "  Year 2017: Found 7603 .DAT file(s)\n",
      "  Year 2017: Cached 60767 records with settlement dates from other years\n",
      "  Year 2017: Matched 200214 records for 2017, cached 60767 for other years\n",
      "  Year 2017: Extracted 200214 records\n",
      "  Year 2017: Date range: 2017-01-01 00:00:00 to 2017-12-15 00:00:00\n",
      "  Year 2017: Saved using fastparquet engine\n",
      "  Year 2017: Saved to ../data/parquet/2017.parquet\n",
      "\n",
      "  Year 2018: Found 6820 .DAT file(s)\n",
      "  Year 2018: Cached 18542 records with settlement dates from other years\n",
      "  Year 2018: Matched 192009 records for 2018, cached 18542 for other years\n",
      "  Year 2018: Extracted 192009 records\n",
      "  Year 2018: Date range: 2018-01-01 00:00:00 to 2018-12-27 00:00:00\n",
      "  Year 2018: Saved using fastparquet engine\n",
      "  Year 2018: Saved to ../data/parquet/2018.parquet\n",
      "\n",
      "  Year 2019: No .DAT files found\n",
      "  Year 2019: Skipped (no data)\n",
      "\n",
      "  Year 2020: Found 6341 .DAT file(s)\n",
      "  Year 2020: Cached 21756 records with settlement dates from other years\n",
      "  Year 2020: Matched 188844 records for 2020, cached 21756 for other years\n",
      "  Year 2020: Extracted 188844 records\n",
      "  Year 2020: Date range: 2020-01-02 00:00:00 to 2020-12-24 00:00:00\n",
      "  Year 2020: Saved using fastparquet engine\n",
      "  Year 2020: Saved to ../data/parquet/2020.parquet\n",
      "\n",
      "  Year 2021: Found 6425 .DAT file(s)\n",
      "  Year 2021: Cached 10628 records with settlement dates from other years\n",
      "  Year 2021: Matched 235028 records for 2021, cached 10628 for other years\n",
      "  Year 2021: Extracted 235028 records\n",
      "  Year 2021: Date range: 2021-01-04 00:00:00 to 2021-12-23 00:00:00\n",
      "  Year 2021: Saved using fastparquet engine\n",
      "  Year 2021: Saved to ../data/parquet/2021.parquet\n",
      "\n",
      "  Year 2022: Found 6419 .DAT file(s)\n",
      "  Year 2022: Cached 2437 records with settlement dates from other years\n",
      "  Year 2022: Matched 193513 records for 2022, cached 2437 for other years\n",
      "  Year 2022: Extracted 193513 records\n",
      "  Year 2022: Date range: 2022-01-03 00:00:00 to 2022-12-22 00:00:00\n",
      "  Year 2022: Saved using fastparquet engine\n",
      "  Year 2022: Saved to ../data/parquet/2022.parquet\n",
      "\n",
      "  Year 2023: Found 6374 .DAT file(s)\n",
      "  Year 2023: Cached 2308 records with settlement dates from other years\n",
      "  Year 2023: Matched 181299 records for 2023, cached 2308 for other years\n",
      "  Year 2023: Extracted 181299 records\n",
      "  Year 2023: Date range: 2023-01-03 00:00:00 to 2023-12-21 00:00:00\n",
      "  Year 2023: Saved using fastparquet engine\n",
      "  Year 2023: Saved to ../data/parquet/2023.parquet\n",
      "\n",
      "  Year 2024: Found 6461 .DAT file(s)\n",
      "  Year 2024: Cached 3743 records with settlement dates from other years\n",
      "  Year 2024: Matched 213114 records for 2024, cached 3743 for other years\n",
      "  Year 2024: Extracted 213114 records\n",
      "  Year 2024: Date range: 2024-01-02 00:00:00 to 2024-12-24 00:00:00\n",
      "  Year 2024: Saved using fastparquet engine\n",
      "  Year 2024: Saved to ../data/parquet/2024.parquet\n",
      "\n",
      "  Year 2025: Found 5740 .DAT file(s)\n",
      "  Year 2025: Cached 1947 records with settlement dates from other years\n",
      "  Year 2025: Matched 207571 records for 2025, cached 1947 for other years\n",
      "  Year 2025: Extracted 207571 records\n",
      "  Year 2025: Date range: 2025-01-02 00:00:00 to 2025-11-20 00:00:00\n",
      "  Year 2025: Saved using fastparquet engine\n",
      "  Year 2025: Saved to ../data/parquet/2025.parquet\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Merging 20 cached record groups back into appropriate year files...\n",
      "Total cached records: 377105\n",
      "  Created 1903.parquet with 1 cached records\n",
      "  Created 1910.parquet with 2 cached records\n",
      "  Created 1911.parquet with 11 cached records\n",
      "  Created 1912.parquet with 5 cached records\n",
      "  Created 1913.parquet with 6 cached records\n",
      "  Created 1914.parquet with 7 cached records\n",
      "  Created 1915.parquet with 6 cached records\n",
      "  Created 1916.parquet with 4 cached records\n",
      "  Created 1955.parquet with 1 cached records\n",
      "  Created 1956.parquet with 2 cached records\n",
      "  Created 1957.parquet with 2 cached records\n",
      "  Created 1958.parquet with 3 cached records\n",
      "  Created 1959.parquet with 3 cached records\n",
      "  Created 1960.parquet with 4 cached records\n",
      "  Created 1961.parquet with 3 cached records\n",
      "  Created 1962.parquet with 5 cached records\n",
      "  Created 1963.parquet with 3 cached records\n",
      "  Created 1964.parquet with 4 cached records\n",
      "  Created 1965.parquet with 5 cached records\n",
      "  Created 1966.parquet with 1 cached records\n",
      "  Created 1967.parquet with 2 cached records\n",
      "  Created 1968.parquet with 6 cached records\n",
      "  Created 1969.parquet with 1 cached records\n",
      "  Created 1971.parquet with 4 cached records\n",
      "  Created 1972.parquet with 4 cached records\n",
      "  Created 1973.parquet with 2 cached records\n",
      "  Created 1974.parquet with 3 cached records\n",
      "  Created 1975.parquet with 15 cached records\n",
      "  Created 1976.parquet with 14 cached records\n",
      "  Created 1977.parquet with 7 cached records\n",
      "  Created 1978.parquet with 5 cached records\n",
      "  Created 1979.parquet with 8 cached records\n",
      "  Created 1980.parquet with 13 cached records\n",
      "  Created 1981.parquet with 8 cached records\n",
      "  Created 1982.parquet with 17 cached records\n",
      "  Created 1983.parquet with 21 cached records\n",
      "  Created 1984.parquet with 19 cached records\n",
      "  Created 1985.parquet with 24 cached records\n",
      "  Created 1986.parquet with 13 cached records\n",
      "  Created 1987.parquet with 32 cached records\n",
      "  Created 1988.parquet with 20 cached records\n",
      "  Created 1989.parquet with 31 cached records\n",
      "  Created 1990.parquet with 29 cached records\n",
      "  Created 1991.parquet with 26 cached records\n",
      "  Created 1992.parquet with 42 cached records\n",
      "  Created 1993.parquet with 52 cached records\n",
      "  Created 1994.parquet with 130 cached records\n",
      "  Created 1995.parquet with 92 cached records\n",
      "  Created 1996.parquet with 207 cached records\n",
      "  Created 1997.parquet with 153 cached records\n",
      "  Created 1998.parquet with 193 cached records\n",
      "  Created 1999.parquet with 258 cached records\n",
      "  Created 2000.parquet with 345 cached records\n",
      "  Created 2001.parquet with 706 cached records\n",
      "  Created 2002.parquet with 922 cached records\n",
      "  Created 2003.parquet with 1754 cached records\n",
      "  Created 2004.parquet with 23670 cached records\n",
      "  Merged 22095 cached records into 2005.parquet (now 181502 total)\n",
      "  Merged 20219 cached records into 2006.parquet (now 182983 total)\n",
      "  Merged 22249 cached records into 2007.parquet (now 209360 total)\n",
      "  Merged 21001 cached records into 2008.parquet (now 191223 total)\n",
      "  Merged 23643 cached records into 2009.parquet (now 200013 total)\n",
      "  Merged 19993 cached records into 2010.parquet (now 181513 total)\n",
      "  Merged 18050 cached records into 2011.parquet (now 172995 total)\n",
      "  Merged 15935 cached records into 2012.parquet (now 172434 total)\n",
      "  Merged 18266 cached records into 2013.parquet (now 197363 total)\n",
      "  Merged 27146 cached records into 2014.parquet (now 227113 total)\n",
      "  Merged 18963 cached records into 2015.parquet (now 224136 total)\n",
      "  Merged 60261 cached records into 2016.parquet (now 268475 total)\n",
      "  Merged 18292 cached records into 2017.parquet (now 218506 total)\n",
      "  Merged 19512 cached records into 2018.parquet (now 211521 total)\n",
      "  Created 2019.parquet with 9835 cached records\n",
      "  Merged 3010 cached records into 2020.parquet (now 191854 total)\n",
      "  Merged 2455 cached records into 2021.parquet (now 237483 total)\n",
      "  Merged 1865 cached records into 2022.parquet (now 195378 total)\n",
      "  Merged 3567 cached records into 2023.parquet (now 184866 total)\n",
      "  Merged 1817 cached records into 2024.parquet (now 214931 total)\n",
      "  Created 2102.parquet with 1 cached records\n",
      "\n",
      "Cache merging complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# process all years and save to parquet\n",
    "print(\"Processing all years...\\n\")\n",
    "\n",
    "cached_records = []\n",
    "\n",
    "for year in YEARS:\n",
    "    df, cache_df = process_year(year, cache_mismatched=True)\n",
    "    \n",
    "    if cache_df is not None and len(cache_df) > 0:\n",
    "        cached_records.append(cache_df)\n",
    "    \n",
    "    if df is not None:\n",
    "        output_file = f\"{OUTPUT_DIR}/{year}.parquet\"\n",
    "        try:\n",
    "            df.to_parquet(output_file, index=False, engine='pyarrow')\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                df.to_parquet(output_file, index=False, engine='fastparquet')\n",
    "                print(f\"  Year {year}: Saved using fastparquet engine\")\n",
    "            except:\n",
    "                df.to_parquet(output_file, index=False)\n",
    "                print(f\"  Year {year}: Saved using auto-detected engine\")\n",
    "        print(f\"  Year {year}: Saved to {output_file}\\n\")\n",
    "    else:\n",
    "        print(f\"  Year {year}: Skipped (no data)\\n\")\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "# Merge cached records back into appropriate year files\n",
    "#! There's a problem here (not too bad) - we have cached records from outside our given timeline (2005-2025). We should filter these out so we don't create .parquet files unnecessarily. Have not done this yet though... just manually deleted.\n",
    "\n",
    "if cached_records:\n",
    "    print(f\"\\nMerging {len(cached_records)} cached record groups back into appropriate year files...\")\n",
    "    \n",
    "    # Combine all cached records\n",
    "    all_cached = pd.concat(cached_records, ignore_index=True)\n",
    "    print(f\"Total cached records: {len(all_cached)}\")\n",
    "    \n",
    "    # Group by settlement date year\n",
    "    if 'settlement_date' in all_cached.columns:\n",
    "        all_cached['settlement_year'] = all_cached['settlement_date'].dt.year\n",
    "        \n",
    "        # Process each year that has cached records\n",
    "        for settlement_year in sorted(all_cached['settlement_year'].unique()):\n",
    "            if pd.isna(settlement_year):\n",
    "                continue\n",
    "            \n",
    "            settlement_year = int(settlement_year)\n",
    "            year_records = all_cached[all_cached['settlement_year'] == settlement_year].copy()\n",
    "            \n",
    "            # Drop the temporary settlement_year column\n",
    "            year_records = year_records.drop(columns=['settlement_year'])\n",
    "            \n",
    "            parquet_file = f\"{OUTPUT_DIR}/{settlement_year}.parquet\"\n",
    "            \n",
    "            if os.path.exists(parquet_file):\n",
    "                # Load existing parquet file\n",
    "                try:\n",
    "                    existing_df = pd.read_parquet(parquet_file, engine='fastparquet')\n",
    "                except:\n",
    "                    existing_df = pd.read_parquet(parquet_file)\n",
    "                # Merge with cached records\n",
    "                merged_df = pd.concat([existing_df, year_records], ignore_index=True)\n",
    "                # Sort by settlement_date\n",
    "                if 'settlement_date' in merged_df.columns:\n",
    "                    merged_df = merged_df.sort_values('settlement_date', na_position='last')\n",
    "                # Save back\n",
    "                try:\n",
    "                    merged_df.to_parquet(parquet_file, index=False, engine='fastparquet')\n",
    "                except:\n",
    "                    merged_df.to_parquet(parquet_file, index=False)\n",
    "                print(f\"  Merged {len(year_records)} cached records into {settlement_year}.parquet (now {len(merged_df)} total)\")\n",
    "            else:\n",
    "                # Create new file if it doesn't exist\n",
    "                if 'settlement_date' in year_records.columns:\n",
    "                    year_records = year_records.sort_values('settlement_date', na_position='last')\n",
    "                try:\n",
    "                    year_records.to_parquet(parquet_file, index=False, engine='fastparquet')\n",
    "                except:\n",
    "                    year_records.to_parquet(parquet_file, index=False)\n",
    "                print(f\"  Created {settlement_year}.parquet with {len(year_records)} cached records\")\n",
    "    \n",
    "    print(\"\\nCache merging complete!\")\n",
    "else:\n",
    "    print(\"\\nNo cached records to merge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "\n",
      " year  total_records  unique_suburbs min_settlement_date max_settlement_date  min_price  max_price  median_price\n",
      " 2005         181502            3470          2005-01-01          2005-12-31        100  710000000      360000.0\n",
      " 2006         182983            3508          2006-01-01          2006-12-31        100  400000000      356500.0\n",
      " 2007         209360            3499          2007-01-01          2007-12-31        100  411000000      370000.0\n",
      " 2008         191223            3520          2008-01-01          2008-12-31        100  770000000      370000.0\n",
      " 2009         200013            3523          2009-01-01          2009-12-31        100  137000000      370000.0\n",
      " 2010         181513            3653          2010-01-01          2010-12-31        100  174000000      415000.0\n",
      " 2011         172995            3649          2011-01-01          2011-12-31        100  395000000      418000.0\n",
      " 2012         172434            3692          2012-01-01          2012-12-31        100  887084627      420000.0\n",
      " 2013         197363            3709          2013-01-01          2013-12-31        100 1755000000      450000.0\n",
      " 2014         227113            3743          2014-01-01          2014-12-31        100  424954046      519000.0\n",
      " 2015         224136            3816          2015-01-01          2015-12-31        100  444679184      548000.0\n",
      " 2016         268475            3850          2016-01-01          2016-12-31        100  445000000      600000.0\n",
      " 2017         218506            3819          2017-01-01          2017-12-31        100  722500000      627200.0\n",
      " 2018         211521            3716          2018-01-01          2018-12-27        100  945945704      655000.0\n",
      " 2019           9835             875          2019-01-03          2019-12-31        100  620951555      660250.0\n",
      " 2020         191854            3692          2020-01-02          2020-12-31        100  530000000      650000.0\n",
      " 2021         237483            3823          2021-01-04          2021-12-31        100  578500000      745000.0\n",
      " 2022         195378            3723          2022-01-03          2022-12-30        100  850000000      820000.0\n",
      " 2023         184866            3593          2023-01-03          2023-12-29        100  410000000      825000.0\n",
      " 2024         214931            3610          2024-01-02          2024-12-31        100  862441438      840000.0\n",
      " 2025         207571            3507          2025-01-02          2025-11-20        100  830000000      900000.0\n"
     ]
    }
   ],
   "source": [
    "# quick summary stats\n",
    "print(\"Summary Statistics:\\n\")\n",
    "\n",
    "summary_data = []\n",
    "for year in YEARS:\n",
    "    parquet_file = f\"{OUTPUT_DIR}/{year}.parquet\"\n",
    "    if os.path.exists(parquet_file):\n",
    "        try:\n",
    "            df = pd.read_parquet(parquet_file, engine='fastparquet')\n",
    "        except:\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "        summary_data.append({\n",
    "            'year': year,\n",
    "            'total_records': len(df),\n",
    "            'unique_suburbs': df['property_locality'].nunique() if 'property_locality' in df.columns else None,\n",
    "            'min_settlement_date': df['settlement_date'].min() if 'settlement_date' in df.columns else None,\n",
    "            'max_settlement_date': df['settlement_date'].max() if 'settlement_date' in df.columns else None,\n",
    "            'min_price': df['purchase_price'].min() if 'purchase_price' in df.columns else None,\n",
    "            'max_price': df['purchase_price'].max() if 'purchase_price' in df.columns else None,\n",
    "            'median_price': df['purchase_price'].median() if 'purchase_price' in df.columns else None,\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "#! remember here that we have the whole NSW. We want to filter this down to just Sydney. We could do so with a suburb / district code list, or preferably a range (if sydney is say between 1000 and 3000 ya know.)\n",
    "\n",
    "#! check out covid (2019)... so few records. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688682a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Checks:\n",
      "\n",
      "Year 2005: No missing values\n",
      "Year 2006: No missing values\n",
      "Year 2007: No missing values\n",
      "Year 2008: No missing values\n",
      "Year 2009: No missing values\n",
      "Year 2010: No missing values\n",
      "Year 2011: No missing values\n",
      "Year 2012: No missing values\n",
      "Year 2013: No missing values\n",
      "Year 2014: No missing values\n",
      "Year 2015: No missing values\n",
      "Year 2016: No missing values\n",
      "Year 2017: No missing values\n",
      "Year 2018: No missing values\n",
      "Year 2019: No missing values\n",
      "Year 2020: No missing values\n",
      "Year 2021: No missing values\n",
      "Year 2022: No missing values\n",
      "Year 2023: No missing values\n",
      "Year 2024: No missing values\n",
      "Year 2025: No missing values\n"
     ]
    }
   ],
   "source": [
    "# checking for missing data\n",
    "print(\"Data Quality Checks:\\n\")\n",
    "\n",
    "for year in YEARS:\n",
    "    parquet_file = f\"{OUTPUT_DIR}/{year}.parquet\"\n",
    "    if os.path.exists(parquet_file):\n",
    "        try:\n",
    "            df = pd.read_parquet(parquet_file, engine='fastparquet')\n",
    "        except:\n",
    "            df = pd.read_parquet(parquet_file)\n",
    "        \n",
    "        missing_settlement = df['settlement_date'].isna().sum() if 'settlement_date' in df.columns else 0\n",
    "        missing_price = df['purchase_price'].isna().sum() if 'purchase_price' in df.columns else 0\n",
    "        missing_suburb = df['property_locality'].isna().sum() if 'property_locality' in df.columns else 0\n",
    "        \n",
    "        if missing_settlement > 0 or missing_price > 0 or missing_suburb > 0:\n",
    "            print(f\"Year {year}:\")\n",
    "            if missing_settlement > 0:\n",
    "                print(f\"  Missing settlement_date: {missing_settlement} ({missing_settlement/len(df)*100:.1f}%)\")\n",
    "            if missing_price > 0:\n",
    "                print(f\"  Missing purchase_price: {missing_price} ({missing_price/len(df)*100:.1f}%)\")\n",
    "            if missing_suburb > 0:\n",
    "                print(f\"  Missing property_locality: {missing_suburb} ({missing_suburb/len(df)*100:.1f}%)\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"Year {year}: No missing values\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
